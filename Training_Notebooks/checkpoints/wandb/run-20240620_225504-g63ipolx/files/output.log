d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
d:\newenv\lib\site-packages\transformers\models\llama\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
{'loss': 2.7136, 'grad_norm': 1.776272177696228, 'learning_rate': 0.00019999959127410606, 'epoch': 0.0}
{'loss': 2.066, 'grad_norm': 1.995184302330017, 'learning_rate': 0.00019999836509976534, 'epoch': 0.0}
{'loss': 1.6006, 'grad_norm': 2.3673996925354004, 'learning_rate': 0.00019999632148700122, 'epoch': 0.0}
