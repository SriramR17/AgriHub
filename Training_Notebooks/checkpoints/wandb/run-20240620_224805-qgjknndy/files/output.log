d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
d:\newenv\lib\site-packages\transformers\models\llama\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
{'loss': 2.7267, 'grad_norm': 1.693223237991333, 'learning_rate': 0.00019999959127410606, 'epoch': 0.0}
{'loss': 2.0845, 'grad_norm': 1.910668134689331, 'learning_rate': 0.00019999836509976534, 'epoch': 0.0}
{'loss': 1.6268, 'grad_norm': 2.680398464202881, 'learning_rate': 0.00019999632148700122, 'epoch': 0.0}
{'loss': 1.1444, 'grad_norm': 1.7396610975265503, 'learning_rate': 0.00019999346045251925, 'epoch': 0.0}
{'loss': 1.1685, 'grad_norm': 1.2978256940841675, 'learning_rate': 0.00019998978201970705, 'epoch': 0.0}
